{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"![Python](https://img.shields.io/badge/Python-3.10%2B-blue) ![Python](https://img.shields.io/badge/Python-3.12%2B%20(Apple%20Silicon)-blue) [![Ollama](https://img.shields.io/badge/Ollama-gray?logo=ollama&amp;logoColor=black&amp;labelColor=white)](hhttps://github.com/ollama/ollama) [![Gradio](https://img.shields.io/badge/Gradio-E76F00?logo=gradio&amp;logoColor=white&amp;)](https://github.com/gradio-app/gradio) [![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE) [![](https://dcbadge.limes.pink/api/server/YuMNeuKStr?style=flat)](https://discord.gg/YuMNeuKStr)  [![Docs](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/docs.yaml/badge.svg)](https://github.com/oronadavid/llm-document-parser/actions/workflows/docs.yaml/) [![Tests](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/tests.yaml/badge.svg)](https://github.com/oronadavid/llm-document-parser/actions/workflows/tests.yaml/) [![Ruff](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/lint.yaml/badge.svg?label=Ruff)](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/lint.yaml/)  [Blueprints Hub](https://developer-hub.mozilla.ai/) | [Documentation](https://mozilla-ai.github.io/llm-document-parser/) | [Getting Started](https://mozilla-ai.github.io/llm-document-parser/getting-started) | [Supported Models](https://mozilla-ai.github.io/llm-document-parser/customization/#supported-models) | [Contributing](CONTRIBUTING.md)"},{"location":"#llm-document-parser-a-blueprint-for-extracting-sturctured-data-from-documents","title":"LLM Document Parser: A Blueprint for extracting sturctured data from documents","text":"<p>This Blueprint provides a locally runnable pipeline for parsing structured data from scanned or digital documents using open-source OCR and LLMs. It takes in one or more documents in image and/or PDF formats as input and returns a single structured object with fields parsed from the documents. By defining a prompt and data model, the Blueprint will know how what fields to parse and what they should look like.</p> <p>The example use case, parsing transaction data from bank statements, demonstrates how you can pass in multiple documents with differing formats and extract shared fields (transaction amount, description, and date). All of the bank statments from every document are compiled into one object. This Blueprint can be customized to work with any type of document to fit your needs.</p> <p></p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#setup","title":"Setup","text":"<pre><code># Clone the repo\ngit clone https://github.com/your-username/llm-document-parser.git\ncd llm-document-parser\n\n# Create a virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # On Windows, use venv\\Scripts\\activate\npip install -e .\n\n# Edit the config with your desired settings and data model(s) using a code editor\nvim src/config.py\n</code></pre>"},{"location":"#graphical-interface-app","title":"Graphical Interface App","text":"<pre><code>python -m llm_document_parser.gradio_app\n</code></pre>"},{"location":"#command-line-interface","title":"Command Line Interface","text":"<pre><code>python -m llm_document_parser.cli\n</code></pre>"},{"location":"#how-it-works","title":"How it Works","text":""},{"location":"#1-image-input","title":"1. Image Input","text":"<ul> <li>Upload scanned digital document images or PDFs</li> </ul>"},{"location":"#2-ocr-model","title":"2. OCR Model","text":"<ul> <li>Input images are passed to an OCR model (Tesseract, EasyOCR, OCR Mac, RapidOCR).</li> <li>The OCR model outputs markdown-formatted text representing the document</li> </ul>"},{"location":"#3-llm-inference","title":"3. LLM Inference","text":"<ul> <li>Text is passed into an instructor-tuned LLM with a user-defined prompt and Pydantic data model</li> <li>The LLM parses and returns a structured JSON with the format specified by the data model</li> </ul>"},{"location":"#4-export","title":"4. Export","text":"<ul> <li>The output can be saved as <code>.json</code> or converted to <code>.csv</code></li> </ul>"},{"location":"#system-requirements","title":"System requirements","text":"<ul> <li>OS: Windows, macOS, or Linux</li> <li>Python 3.10 or higher</li> <li>Minimum RAM: 8 GB</li> <li>Disk space: 6 GB minimum</li> <li>GPU (optional): a GPU will enable the use of more powerful LLMs. 4GB+ of VRAM is recommended if using a GPU</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache 2.0 License. See the LICENSE file for details.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! To get started, you can check out the CONTRIBUTING.md file.</p>"},{"location":"api/","title":"\ud83e\udde9 API Reference","text":"<p>This page is automatically generated from Python docstrings using mkdocstrings.</p>"},{"location":"api/#ocr-pipeline","title":"OCR Pipeline","text":""},{"location":"api/#llm_document_parser.convert_doc_docling.load_rapid_ocr_model","title":"<code>llm_document_parser.convert_doc_docling.load_rapid_ocr_model(det_model, rec_model, cls_model)</code>","text":"<p>Load the RapidOCR model from Hugging Face Hub. Args:     det_model (str): Path to the detection model.     rec_model (str): Path to the recognition model.     cls_model (str): Path to the classification model. Returns:     DocumentConverter: The loaded RapidOCR model.</p> Source code in <code>src/llm_document_parser/convert_doc_docling.py</code> <pre><code>def load_rapid_ocr_model(det_model: str, rec_model: str, cls_model: str) -&gt; DocumentConverter:\n    \"\"\"\n    Load the RapidOCR model from Hugging Face Hub.\n    Args:\n        det_model (str): Path to the detection model.\n        rec_model (str): Path to the recognition model.\n        cls_model (str): Path to the classification model.\n    Returns:\n        DocumentConverter: The loaded RapidOCR model.\n    \"\"\"\n    print(\"Downloading RapidOCR models\")\n    download_path = snapshot_download(repo_id=\"SWHL/RapidOCR\")\n\n    det_model_path = os.path.join(\n        download_path, det_model\n    )\n    rec_model_path = os.path.join(\n        download_path, rec_model\n    )\n    cls_model_path = os.path.join(\n        download_path, cls_model\n    )\n\n    ocr_options = RapidOcrOptions(\n        det_model_path=det_model_path,\n        rec_model_path=rec_model_path,\n        cls_model_path=cls_model_path\n    )\n\n    pipeline_options = PdfPipelineOptions(\n        ocr_options=ocr_options\n    )\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.IMAGE: ImageFormatOption(\n                pipeline_options=pipeline_options\n            )\n        }\n    )\n\n    return doc_converter\n</code></pre>"},{"location":"api/#llm_document_parser.convert_doc_docling.image_to_text","title":"<code>llm_document_parser.convert_doc_docling.image_to_text(document_converter, file_path)</code>","text":"<p>Convert an image to text using the specified document converter. Args:     document_converter (DocumentConverter): The document converter to use.     file_path (Path): Path to the image file. Returns:     ConversionResult: The result of the conversion.</p> Source code in <code>src/llm_document_parser/convert_doc_docling.py</code> <pre><code>def image_to_text(document_converter: DocumentConverter, file_path: Path) -&gt; ConversionResult:\n    \"\"\"\n    Convert an image to text using the specified document converter.\n    Args:\n        document_converter (DocumentConverter): The document converter to use.\n        file_path (Path): Path to the image file.\n    Returns:\n        ConversionResult: The result of the conversion.\n    \"\"\"\n    conv_results = document_converter.convert(file_path)\n    return conv_results\n</code></pre>"},{"location":"api/#llm-processing","title":"LLM Processing","text":""},{"location":"api/#llm_document_parser.instructor_llm.extract_json_data_using_ollama_llm","title":"<code>llm_document_parser.instructor_llm.extract_json_data_using_ollama_llm(prompt, text_data, ollama_model, response_model)</code>","text":"<p>Pass prompt and data into an ollama LLM using instructor</p> Source code in <code>src/llm_document_parser/instructor_llm.py</code> <pre><code>def extract_json_data_using_ollama_llm(prompt: str, text_data: str, ollama_model: str, response_model: Type[BaseModel]) -&gt; str:\n    \"\"\"\n    Pass prompt and data into an ollama LLM using instructor\n    \"\"\"\n    client = instructor.from_openai(\n        OpenAI(\n            base_url=\"http://localhost:11434/v1\",\n            api_key=\"ollama\"\n        ),\n        mode=instructor.Mode.JSON\n    )\n\n    resp = client.chat.completions.create(\n        model=ollama_model,\n        messages=[\n            {\n                'role': 'system',\n                'content': prompt\n            },\n            {\n                'role': 'user',\n                'content': text_data\n            },\n        ],\n        response_model=response_model,\n        max_retries=3\n    )\n\n    return resp.model_dump_json(indent=4)\n</code></pre>"},{"location":"api/#exporting","title":"Exporting","text":""},{"location":"api/#llm_document_parser.export_data.export_as_csv","title":"<code>llm_document_parser.export_data.export_as_csv(df, output_folder, output_file_name)</code>","text":"<p>Save a DataFrame as a CSV file, avoiding overwriting by incrementing filenames.</p> Source code in <code>src/llm_document_parser/export_data.py</code> <pre><code>def export_as_csv(df: pd.DataFrame, output_folder: str, output_file_name: str) -&gt; str:\n    \"\"\"\n    Save a DataFrame as a CSV file, avoiding overwriting by incrementing filenames.\n    \"\"\"\n    output_folder_path = Path(output_folder)\n    if not output_folder_path.is_dir():\n        print(f\"Creating path {output_folder}\")\n        output_folder_path.mkdir(parents=True)\n\n    file_index = 0\n    while True:\n        full_output_path = output_folder_path / f\"{output_file_name}{file_index}.csv\"\n        if not full_output_path.exists():\n            break\n        file_index += 1\n\n    df.to_csv(full_output_path, index=False)\n    print(f\"Saved CSV to {full_output_path}\")\n    return df.to_csv(path_or_buf=None, index=False)\n</code></pre>"},{"location":"api/#llm_document_parser.export_data.export_as_json","title":"<code>llm_document_parser.export_data.export_as_json(df, output_folder, output_file_name)</code>","text":"<p>Save raw JSON string to a file, avoiding overwriting by incrementing filenames.</p> Source code in <code>src/llm_document_parser/export_data.py</code> <pre><code>def export_as_json(df: pd.DataFrame, output_folder: str, output_file_name: str) -&gt; str:\n    \"\"\"\n    Save raw JSON string to a file, avoiding overwriting by incrementing filenames.\n    \"\"\"\n    output_folder_path = Path(output_folder)\n    if not output_folder_path.is_dir():\n        print(f\"Creating path {output_folder}\")\n        output_folder_path.mkdir(parents=True)\n\n    file_index = 0\n    while True:\n        full_output_path = output_folder_path / f\"{output_file_name}{file_index}.json\"\n        if not full_output_path.exists():\n            break\n        file_index += 1\n\n    df.to_json(full_output_path, orient='records')\n    print(f\"Saved JSON to {full_output_path}\")\n    return df.to_json(orient='records') or \"\"\n</code></pre>"},{"location":"api/#llm_document_parser.export_data.convert_json_to_df","title":"<code>llm_document_parser.export_data.convert_json_to_df(json_data)</code>","text":"<p>Convert a JSON string into a pandas DataFrame. Automatically extracts the first top-level list if present.</p> Source code in <code>src/llm_document_parser/export_data.py</code> <pre><code>def convert_json_to_df(json_data: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a JSON string into a pandas DataFrame.\n    Automatically extracts the first top-level list if present.\n    \"\"\"\n    data = json.loads(json_data)\n\n    # Try to extract the list of transactions if it's wrapped\n    list_name = None\n    for key, value in data.items():\n        if isinstance(value, list):\n            list_name = key\n            break\n\n    if list_name:\n        data = data[list_name]\n\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"badged-header/","title":"Badged header","text":"![Python](https://img.shields.io/badge/Python-3.10%2B-blue) ![Python](https://img.shields.io/badge/Python-3.12%2B%20(Apple%20Silicon)-blue) [![Ollama](https://img.shields.io/badge/Ollama-gray?logo=ollama&amp;logoColor=black&amp;labelColor=white)](https://github.com/ggml-org/llama.cpp) [![Gradio](https://img.shields.io/badge/Gradio-E76F00?logo=gradio&amp;logoColor=white)](https://streamlit.io/) [![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE) [![](https://dcbadge.limes.pink/api/server/YuMNeuKStr?style=flat)](https://discord.gg/YuMNeuKStr)  [![Docs](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/docs.yaml/badge.svg)](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/docs.yaml/) [![Tests](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/tests.yaml/badge.svg)](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/tests.yaml/) [![Ruff](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/lint.yaml/badge.svg?label=Ruff)](https://github.com/mozilla-ai/document-to-podcast/actions/workflows/lint.yaml/)  [Blueprints Hub](https://developer-hub.mozilla.ai/) | [Documentation](https://mozilla-ai.github.io/document-to-podcast/) | [Getting Started](https://mozilla-ai.github.io/document-to-podcast/getting-started) | [Supported Models](https://mozilla-ai.github.io/document-to-podcast/customization/#supported-models) | [Contributing](CONTRIBUTING.md)"},{"location":"badged-header/#llm-document-parser-a-blueprint-for-extracting-sturctured-data-from-documents","title":"LLM Document Parser: A Blueprint for extracting sturctured data from documents","text":"<p>This Blueprint provides a modular, locally runnable pipeline for parsing structured data (like bank transactions) from scanned or digital documents using open-source OCR and LLM tools.</p> <p>Whether you're working with bank statements, receipts, or other sensitive documents, this system ensures data privacy and full customization\u2014without relying on proprietary APIs.</p> <p></p>"},{"location":"badged-header/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"badged-header/#setup","title":"Setup","text":""},{"location":"badged-header/#prepare-the-project","title":"Prepare the Project","text":"<p><pre><code># Clone the repo\ngit clone https://github.com/your-username/llm-document-parser.git\ncd llm-document-parser\n\n# Create a virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # On Windows, use venv\\Scripts\\activate\npip install -e .\n\n# Edit the config with your desired settings and data model(s) using a code editor\nvim src/config.py\n</code></pre> MOVE OUT OF QUICK START v</p>"},{"location":"badged-header/#install-tesseractocr-optional-if-using-tesseract-for-ocr","title":"Install TesseractOCR (optional - if using Tesseract for OCR)","text":"<p>Tesseract Installation Documentation</p>"},{"location":"badged-header/#ubuntu","title":"Ubuntu","text":"<pre><code>sudo apt install tesseract-ocr\n</code></pre>"},{"location":"badged-header/#macos","title":"MacOS","text":"<pre><code># MacPorts\nsudo port install tesseract\n\n# Homebrew\nbrew install tesseract\n</code></pre>"},{"location":"badged-header/#windows","title":"Windows","text":"<p>Download the installer</p> <p>MOVE ^</p>"},{"location":"badged-header/#creating-a-data-model","title":"Creating a Data Model","text":"<p>Docling uses Pydantic to force the LLM output to conform to an exact data model. In order to </p>"},{"location":"badged-header/#graphical-interface-app","title":"Graphical Interface App","text":"<pre><code>python -m llm_document_parser.gradio_app\n</code></pre>"},{"location":"badged-header/#command-line-interface","title":"Command Line Interface","text":"<pre><code>python -m llm_document_parser.cli\n</code></pre>"},{"location":"badged-header/#docsstep-by-step-guidemd","title":"\ud83d\udcc4 <code>docs/step-by-step-guide.md</code>","text":""},{"location":"badged-header/#how-it-works","title":"How it Works","text":""},{"location":"badged-header/#1-image-input","title":"1. Image Input","text":"<ul> <li>Upload scanned or digital document images through the Gradio UI.</li> </ul>"},{"location":"badged-header/#2-ocr-model","title":"2. OCR Model","text":"<ul> <li>Image is passed to an OCR model (Docling, Tesseract, or PP-OCRv4).</li> <li>Output is raw unstructured text.</li> </ul>"},{"location":"badged-header/#3-llm-inference","title":"3. LLM Inference","text":"<ul> <li>Text is sent to an instructor-tuned LLM via <code>extract_json_data_using_ollama_llm()</code>.</li> <li>The LLM parses and returns structured JSON.</li> </ul>"},{"location":"badged-header/#4-export","title":"4. Export","text":"<ul> <li>JSON is optionally saved as <code>.json</code> or converted to <code>.csv</code> using <code>export_data.py</code> utilities.</li> </ul>"},{"location":"badged-header/#system-requirements","title":"System requirements","text":"<ul> <li>OS: Windows, macOS, or Linux</li> <li>Python 3.10 or higher</li> <li>Minimum RAM: 8 GB</li> <li>Disk space: 6 GB minimum</li> <li>GPU (optional): a GPU will enable the use of more powerful LLMs. 4GB+ of VRAM is recommended if using a GPU</li> </ul>"},{"location":"customization/","title":"Customization Guide","text":"<p>Modify and tailor the blueprint to your specific needs.</p>"},{"location":"customization/#swap-ocr-models","title":"Swap OCR Models","text":"<p>In <code>convert_doc_docling.py</code>, choose one of:</p> <ul> <li><code>load_rapid_ocr_model()</code></li> <li><code>load_easy_ocr_model()</code></li> <li><code>load_ocr_mac_model()</code></li> </ul>"},{"location":"customization/#use-a-different-llm","title":"Use a Different LLM","text":"<p>In <code>instructor_llm.py</code>:</p> <ul> <li>Change the system prompt in <code>config.py</code></li> <li>Use a different model with Ollama (e.g., <code>phi</code>, <code>llama3</code>, <code>dolphin3</code>)</li> </ul>"},{"location":"customization/#adjust-output","title":"Adjust Output","text":"<ul> <li>Modify the output schema returned by the LLM</li> <li>Customize <code>export_as_csv()</code> or <code>convert_json_to_df()</code> to fit your format</li> </ul>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>This Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you\u2019re an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#enhance-the-blueprint","title":"\ud83d\udee0\ufe0f Enhance the Blueprint","text":"<ul> <li>Check the Issues page to see if there are feature requests you'd like to implement</li> <li>Refer to our Contribution Guide for more details on contributions</li> </ul>"},{"location":"future-features-contributions/#extensibility-ideas","title":"\ud83c\udfa8 Extensibility Ideas","text":"<p>This Blueprint is designed to be a foundation you can build upon. By extending its capabilities, you can open the door to new applications, improve user experience, and adapt the Blueprint to address other use cases. Here are a few ideas for how you can expand its potential:</p> <p>We\u2019d love to see how you can enhance this Blueprint! If you create improvements or extend its capabilities, consider contributing them back to the project so others in the community can benefit from your work. Check out our Contributions Guide to get started!</p>"},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got an idea for how this Blueprint could be improved? You can share your suggestions through GitHub Discussions.</p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you\u2019re inspired to create your own Blueprint, you can use the Blueprint-template to get started.</p> <p>Your contributions help make this Blueprint better for everyone \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prepare-the-project","title":"Prepare the Project","text":"<pre><code># Clone the repo\ngit clone https://github.com/your-username/llm-document-parser.git\ncd llm-document-parser\n\n# Create a virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # On Windows, use venv\\Scripts\\activate\npip install -e .\n\n# Edit the config with your desired settings and data model(s) using a code editor\nvim src/config.py\n</code></pre>"},{"location":"getting-started/#install-tesseractocr-optional-if-using-tesseract-for-ocr","title":"Install TesseractOCR (optional - if using Tesseract for OCR)","text":"<p>Tesseract Installation Documentation</p>"},{"location":"getting-started/#ubuntu","title":"Ubuntu","text":"<pre><code>sudo apt install tesseract-ocr\n</code></pre>"},{"location":"getting-started/#macos","title":"MacOS","text":"<pre><code># MacPorts\nsudo port install tesseract\n\n# Homebrew\nbrew install tesseract\n</code></pre>"},{"location":"getting-started/#windows","title":"Windows","text":"<p>Download the installer</p> <p>MOVE ^</p>"},{"location":"getting-started/#creating-a-data-model","title":"Creating a Data Model","text":"<p>Docling uses Pydantic to force the LLM output to conform to an exact data model. In order to </p>"},{"location":"getting-started/#graphical-interface-app","title":"Graphical Interface App","text":"<pre><code>python -m llm_document_parser.gradio_app\n</code></pre>"},{"location":"getting-started/#command-line-interface","title":"Command Line Interface","text":"<pre><code>python -m llm_document_parser.cli\n</code></pre>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide","text":""},{"location":"step-by-step-guide/#docsstep-by-step-guidemd","title":"\ud83d\udcc4 <code>docs/step-by-step-guide.md</code>","text":"<p>```markdown</p>"},{"location":"step-by-step-guide/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>This guide walks through how the document parsing pipeline works internally.</p>"},{"location":"step-by-step-guide/#1-image-input","title":"1. Image Input","text":"<ul> <li>Users upload scanned or digital document images through the Gradio UI.</li> </ul>"},{"location":"step-by-step-guide/#2-ocr-model","title":"2. OCR Model","text":"<ul> <li>Image is passed to an OCR model (Docling, Tesseract, or PP-OCRv4).</li> <li>Output is raw unstructured text.</li> </ul>"},{"location":"step-by-step-guide/#3-llm-inference","title":"3. LLM Inference","text":"<ul> <li>Text is sent to an instructor-tuned LLM via <code>extract_json_data_using_ollama_llm()</code>.</li> <li>The LLM parses and returns structured JSON.</li> </ul>"},{"location":"step-by-step-guide/#4-export","title":"4. Export","text":"<ul> <li>JSON is optionally saved as <code>.json</code> or converted to <code>.csv</code> using <code>export_data.py</code> utilities.</li> </ul>"}]}